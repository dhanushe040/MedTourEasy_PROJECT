{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a331e358-f9b3-4db1-b08d-3f3f7cd037db",
   "metadata": {},
   "source": [
    "Blood Donor Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d00258-571b-4b58-9d8a-0ae938cf9c02",
   "metadata": {},
   "source": [
    "Step 1:Blood Donor Prediction: Downloading Dataset with KaggleHub"
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {},
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"ninalabiba/blood-transfusion-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8f0628af-e05a-4607-853c-ac33a776eb4f",
   "metadata": {},
   "source": [
    "Step 2: Read Blood Donation Data with Pandas"
   ]
  },
  {
   "cell_type": "code",
   "id": "43e3c49a-860f-46c0-affe-da8ddb58ce81",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "transfusion=pd.read_csv(r\"C:\\Users\\hp\\.cache\\kagglehub\\datasets\\ninalabiba\\blood-transfusion-dataset\\versions\\transfusion.csv\")\n",
    "transfusion"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e7d93ddc-3de9-4915-b6c8-97915b07f6e3",
   "metadata": {},
   "source": [
    "### ✅ Task 1 : Inspecting the Dataset\n",
    "\n",
    "Using the shell command `!head datasets/transfusion.data`, I printed the first 5 lines of the dataset to verify its structure and contents. This initial inspection helped confirm the format and column arrangement before loading the data with pandas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cdbf16-5465-4b40-87eb-1d0eb106b1f2",
   "metadata": {},
   "source": [
    "!head -n5 datasets/transfusion.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2ea598-e092-4699-835d-f9b917ddcb19",
   "metadata": {},
   "source": [
    "### ✅ Task 2 : Loading the Dataset\n",
    "\n",
    "I imported the pandas library and successfully loaded the `transfusion.csv` file into a DataFrame named `transfusion`. Using `head()`, I verified that the dataset contains 5 columns and 749 rows, confirming it was loaded correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "d4a1634e-686b-4c80-ba02-459e73b49289",
   "metadata": {},
   "source": [
    "transfusion.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9bf12f6f-f7d7-46ed-830b-2f7362f79ba1",
   "metadata": {},
   "source": [
    "### ✅ Task 3 : Inspecting DataFrame Structure\n",
    "\n",
    "Using the `info()` method, I examined the structure of the `transfusion` DataFrame. It confirmed that all 5 columns are non-null, the data types are appropriate for analysis, and the dataset contains 749 entries. This step ensures the data is ready for preprocessing.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "8aef5c96-dee5-4c4d-a581-964c8a526ba1",
   "metadata": {},
   "source": [
    "transfusion.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1b6250ef-20b2-4536-a266-347c9e50722e",
   "metadata": {},
   "source": [
    "### ✅ Task 4 : Renaming Target Column\n",
    "\n",
    "Renamed the column `'whether he/she donated blood in March 2007'` to `'target'` for brevity and clarity using `rename()` with `inplace=True`. Verified the change by printing the first two rows with `head(2)`, confirming the updated column name is now reflected in the DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "abfa8ef7-e55b-4e40-baf7-9b532f975b79",
   "metadata": {},
   "source": [
    "transfusion.rename(columns={\"whether he/she donated blood in March 2007\":\"target\"},inplace=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8c6c95cf-6248-4dae-a7f7-778ccb737194",
   "metadata": {},
   "source": [
    "transfusion.head(2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "70d65539-2332-483c-b5eb-308757aa9b2d",
   "metadata": {},
   "source": [
    "### ✅ Task 5 : Target Incidence Analysis\n",
    "\n",
    "Used `value_counts(normalize=True)` on the `transfusion.target` column to calculate the proportion of donors vs. non-donors. Rounded the output to 3 decimal places for clarity. This step helps understand the class distribution and highlights any imbalance in the target variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "2d90375d-9db4-494e-8652-dd8fc2d7045f",
   "metadata": {},
   "source": [
    "transfusion.target.value_counts(normalize=True).round(3)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "86f91e55-d7e8-46dc-9b79-64996ac0643c",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c450769c-bb70-431b-b418-1228595d1bef",
   "metadata": {},
   "source": [
    "### ✅ Task 6 : Splitting Data for Model Training\n",
    "\n",
    "Used `train_test_split()` from `sklearn.model_selection` to divide the `transfusion` DataFrame into training and testing sets:\n",
    "- Features (`X`) and target (`y`) were separated.\n",
    "- Stratified sampling ensured balanced class distribution.\n",
    "- 75% of the data was used for training, 25% for testing.\n",
    "- Verified the split by printing the first two rows of `X_train`.\n",
    "\n",
    "This prepares the data for model building while maintaining label proportions.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "15f5cff1-f3bb-48b0-a3d4-8b44812e198e",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import train_test_split"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6d7cd0af-61dd-456e-a17b-6f717e281482",
   "metadata": {},
   "source": [
    "x_train,X_test,y_train,y_test = train_test_split(\n",
    "    transfusion.drop(columns='target'),\n",
    "    transfusion.target,\n",
    "    test_size=0.25,\n",
    "    random_state=42,\n",
    "    stratify= transfusion.target \n",
    ")\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e1c27aa8-5741-48a2-978d-79117e1b8493",
   "metadata": {},
   "source": [
    "x_train.head(2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f01d01e9-d6ae-4a9b-b51f-73fe5a9f8960",
   "metadata": {},
   "source": [
    "### ✅ Task 7 : TPOT Pipeline Optimization\n",
    "\n",
    "Used `TPOTClassifier` to automatically discover the best machine learning pipeline:\n",
    "- Optimized for `roc_auc` score to evaluate model performance.\n",
    "- Set `random_state=42` for reproducibility.\n",
    "- Trained using `.fit()` and evaluated with `roc_auc_score`.\n",
    "- Displayed pipeline steps using `tpot.fitted_pipeline_.steps`.\n",
    "\n",
    "This approach helped identify the most effective combination of preprocessing and modeling techniques for predicting blood donation.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "f5c0aacb-828b-4f88-86bd-8eae876d596a",
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "from tpot import TPOTClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Create TPOTClassifier instance with minimal arguments\n",
    "tpot = TPOTClassifier(\n",
    "    generations=5,\n",
    "    population_size=20,\n",
    "    verbose=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit TPOT on training data\n",
    "tpot.fit(x_train, y_train)\n",
    "\n",
    "# Predict probabilities and calculate AUC score\n",
    "y_pred_proba = tpot.predict_proba(X_test)[:, 1]\n",
    "tpot_auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "print(\"TPOT AUC Score:\", round(tpot_auc_score, 4))\n",
    "\n",
    "# Display pipeline steps\n",
    "print(\"\\nBest pipeline steps:\")\n",
    "for idx, transform in tpot.fitted_pipeline_.steps:\n",
    "    print(f\"{idx}: {transform}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b4260b68-486b-4fbd-ada4-a77a94427725",
   "metadata": {},
   "source": [
    "### ✅ Task 8 : Feature Variance Check\n",
    "\n",
    "Used `pandas.DataFrame.var()` to calculate column-wise variance in `X_train`:\n",
    "- Rounded results to 3 decimal places for readability.\n",
    "- This helps identify which features vary the most and may influence model performance.\n",
    "\n",
    "A useful step for understanding feature distribution before scaling or selection.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "065c6a02-30c7-424d-a268-6c789c5d3028",
   "metadata": {},
   "source": "x_train.var().round(3)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "70c20efc-8c4d-402b-96a9-f2de0192cf61",
   "metadata": {},
   "source": [
    "### ✅ Task 9 : Correcting High Variance with Log Normalization\n",
    "\n",
    "Identified the feature with the highest variance and applied log normalization to reduce its impact:\n",
    "- Copied `X_train` and `X_test` into `X_train_normed` and `X_test_normed`.\n",
    "- Used a for-loop to apply the same transformation to both datasets.\n",
    "- Replaced the high-variance column with its log-normalized version.\n",
    "- Verified the change by printing the updated variance, rounded to 3 decimal places.\n",
    "\n",
    "This step helps stabilize feature scales and may improve model performance.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Import numpy\n",
    "import numpy as np\n",
    "\n",
    "# Copy X_train and X_test into X_train_normed and X_test_normed\n",
    "X_train_normed,X_test_normed  = x_train.copy(), X_test.copy()\n",
    "\n",
    "# Specify which column to normalize\n",
    "col_to_normalize = 'Monetary (c.c. blood)'\n",
    "\n",
    "# Log normalization\n",
    "for df_ in [X_train_normed, X_test_normed]:\n",
    "    # Add log normalized column\n",
    "    df_['monetary_log'] = np.log(df_['Monetary (c.c. blood)'])\n",
    "    # Drop the original column\n",
    "    df_.drop(columns='Monetary (c.c. blood)', inplace=True)\n",
    "\n",
    "# Check the variance for X_train_normed\n",
    "X_train_normed.var().round(3)"
   ],
   "id": "cf54b8997259eb2f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "96bbd1037111c186"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ✅ Task 10 : Training Logistic Regression Model\n",
    "\n",
    "Trained a logistic regression model using scikit-learn:\n",
    "- Imported `LogisticRegression` from `sklearn.linear_model`.\n",
    "- Created an instance and trained it using `.fit()` on the normalized training data.\n",
    "- Evaluated performance using `roc_auc_score` and printed the result.\n",
    "\n",
    "This step provides a baseline model for comparison with TPOT’s optimized pipeline."
   ],
   "id": "1e30202ef7bfed1d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T07:27:22.313201Z",
     "start_time": "2025-08-19T07:27:21.973145Z"
    }
   },
   "cell_type": "code",
   "source": "from sklearn.metrics import roc_auc_score\n",
   "id": "1d0c93ec2895d7d4",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T07:27:27.641960Z",
     "start_time": "2025-08-19T07:27:26.629232Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Importing modules\n",
    "from sklearn import linear_model\n",
    "\n",
    "# Instantiate LogisticRegression\n",
    "logreg = linear_model.LogisticRegression(\n",
    "    solver='liblinear',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "logreg.fit(X_train_normed, y_train)\n",
    "\n",
    "# AUC score for tpot model\n",
    "logreg_auc_score = roc_auc_score(y_test, logreg.predict_proba(X_test_normed)[:, 1])\n",
    "print(f'\\nAUC score: {logreg_auc_score:.4f}')"
   ],
   "id": "e68c9182bcee60be",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AUC score: 0.7891\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ✅ Task 11 : Sorting Models by AUC Score\n",
    "\n",
    "Sorted models based on their AUC scores:\n",
    "- Imported `itemgetter` from the `operator` module.\n",
    "- Created a list of `(model_name, model_score)` pairs.\n",
    "- Sorted the list in descending order using `sorted(..., reverse=True)`.\n",
    "\n",
    "This step helps identify the best-performing model and supports informed model selection.\n",
    "\n"
   ],
   "id": "f66fa3ef29f1cb6f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Importing itemgetter\n",
    "from operator import itemgetter\n",
    "\n",
    "# Sort models based on their AUC score from highest to lowest\n",
    "sorted(\n",
    "    [('tpot', tpot_auc_score), ('logreg', logreg_auc_score)],\n",
    "    key=itemgetter(1),\n",
    "    reverse =True"
   ],
   "id": "31006352c94e6904"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
